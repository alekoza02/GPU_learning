% \documentclass[demo]{article}
\documentclass{article}
\usepackage{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage[english]{babel}
\usepackage{braket}
\usepackage{cancel}

\usepackage{listings}
\usepackage{xcolor}

\lstset{
  language=Python,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  stringstyle=\color{orange},
  commentstyle=\color{gray},
  numbers=left,
  numberstyle=\tiny\color{gray},
  stepnumber=1,
  numbersep=5pt,
  backgroundcolor=\color{white},
  showspaces=false,
  showstringspaces=false,
  frame=single,
  breaklines=true,
  captionpos=b
}

\usepackage{hyperref}

\setcounter{page}{0}

\title{CUDA learning}
\author{Alessio Cimma}

\begin{document}

\maketitle
\newpage

\tableofcontents
\newpage

\section{Project structure}

The program will contain a \texttt{Bridge} class, which will help call different functions during training. The idea is to have a comfortable envoirement to learn new things. I'll add in near future new tools to make debugging, plotting, benchamrking and comparisons easier. Each exercise will be contained in a different \texttt{self.execute\_exerciseX()}. At the beginning of the program you will be asked to choose which CUDA kernel and function to load. Change it each time you want test different exercises. Possibly, each kernel will contain only one exercise, if the exercise needs more than one function they will all be contained in a single \texttt{.cu} file.

\begin{lstlisting}[caption=Loading CUDA kernel (kernel.cu)]
# Leggi il file contenente il kernel CUDA
with open(f'{kernel_name}.cu', 'r') as f:
    kernel_code = f.read()
\end{lstlisting}

\newpage
\section{CUDA architecture}

\subsection{Hierarchy of CUDA architecture}
We have a grid, which determines the number of blocks, which are a collection of threads. A grid 2x2 contains 4 blocks. If each block is 32 threads, we will have 128 threads total.

To determine the optimal amount of grid and blocks, organize the number of blocks and then derive the size of the grid using the following subsections.

\subsection{How to launch a CUDA kernel} Remember that when launching the kernel, the first 2 arguments need to be passed as tuple (grid\_size and block\_size), even if it's just one number, to do it use the syntax \texttt{(x, )}:
\begin{lstlisting}[caption=Launch CUDA kernel]
# 1D version
self.imported_kernel((grid_size,), (block_size,), (x, y, z, n)) 

# 2D version
self.imported_kernel(grid_size, block_size, (x, y, z, n)) 
\end{lstlisting}

\newpage

\subsection{Best parameters (block\_size)} The following statistics should help choosing the best block-size to use in future projects, remember that you can't exceed the \textbf{MaxThreadsPerBlock} value (1024 on my laptop):
\begin{itemize}
    \item Memory-bound kernels (loads/stores): 256-1024 $\longrightarrow$ \textit{Hide latency via massive parallelism}
    \item Compute-bound kernels: 128-512 $\longrightarrow$ \textit{More registers per thread may be needed}
    \item Shared memory usage per block: 128-256 $\longrightarrow$ \textit{Avoid limiting number of resident blocks}
    \item Register-heavy kernels: 128 or lower $\longrightarrow$ \textit{Prevent spilling and reduce pressure}
    \item Small data sizes (n $<$ 10k): 64-256 $\longrightarrow$ \textit{Larger blocks may cause underutilization}
    \item Thread divergence (if-else logic): 32-64 $\longrightarrow$ \textit{Keep warps smaller to minimize divergence waste}
\end{itemize}


\subsection{Best parameters (grid\_size)} The best grid size you can use is the following, it maximizes the usage of the GPU by dividing the problem equally across all blocks:
\begin{lstlisting}[caption=Determine grid size]
# 1D version
grid_size = (n + block_size - 1) // block_size 

# 2D version
grid_size = (
  (W_delta + block_size[0] - 1) // block_size[0],
  (W_delta + block_size[1] - 1) // block_size[1]
) 
\end{lstlisting}

\newpage

\section{Exercise 1}
\paragraph{Objective:}
Creating a kernel that sums two 1D arrays element-wise.
\paragraph{What I learned:}
\begin{itemize}
  \item To access the correct index at which I am working, I need to use:
  \begin{lstlisting}[caption=Index access]
int idx = threadIdx.x + blockIdx.x * blockDim.x;
  \end{lstlisting}
  This way, I enter the correct block position multiplied by the size of the block (measured in threads) and then add the thread index of its parent block.
  \item  I should the following command to cover the edge case where i have a potential thread that can do some work, but there's no more work to be found (due to the size of the work not being a power of two):
  \begin{lstlisting}[caption=Index edge-case]
if (idx < n)
  \end{lstlisting}
\end{itemize}

\newpage
\section{Exercise 2}
\paragraph{Objective:}
Creating a kernel that performs a convolution (morphological erosion).
\paragraph{What I learned:}
  
\begin{itemize}
  \item If the grid is 2D I can access the indices X and Y of the block and thread:
    \begin{lstlisting}[caption=Index access]
int i = blockIdx.y * blockDim.y + threadIdx.y;
int j = blockIdx.x * blockDim.x + threadIdx.x;
    \end{lstlisting}
  \item Now to check the new boundaries:
    \begin{lstlisting}[caption=Index edge-case]
if (i >= Size_x || j >= Size_y) return;
    \end{lstlisting}
    \item I can obtain the value of infinity without importing header files using:
    \begin{lstlisting}[caption=Infinity]
float min_val = 1.0f / 0.0f;
    \end{lstlisting}
\end{itemize}

\newpage

\section{Exercise 3}
\paragraph{Objective:}
Trying to speeding up the execution time of the Exercise 2 using shared memory. 

\begin{table}[h!]
  \centering
  \begin{tabular}{c|c|c}
  \textbf{Execution Time} & \textbf{NO shared-memory} & \textbf{shared-memory} \\
  \hline
  & 75.3s & 84.7s \\
  \end{tabular}
  \caption{Performance comparison}
  \label{tab:simple_table}
\end{table}

\paragraph{What I learned:}
No need for shared memory for data that needs to be accessed from ALL threads just once. Useful when each thread needs to access a certain value multiple time during the execution. The best case is when an entire block needs a batch of data to use multiple times, otherwise the memory overhead and transfer time makes it slower than just accessing the data from global memory.

\paragraph{Cooperative-loading:} Interesting way of loading memory using all threads:

\begin{lstlisting}[caption=Index edge-case]
for (int idx = local_thread_idx; idx < total_tile_size; idx += threads_per_block) {
    int ki = idx / TILE_KW;
    int kj = idx % TILE_KW;

    int global_ki = tile_offset_i + ki;
    int global_kj = tile_offset_j + kj;

    float val = (global_ki < KH && global_kj < KW)
        ? ker[global_ki * KW + global_kj] : 0.0f;

    ker_tile[ki * (TILE_KW + 1) + kj] = val;
}
\end{lstlisting}

\newpage
\section{Exercise 4 / 5}

\paragraph{Objective:}

Building a simple UV mapping and building a very simple shader. Visualize the result in pygame in real-time.

\paragraph{What I learned:}
\begin{itemize}
  \item To increase performance you can use this to avoid branching:
  \begin{lstlisting}[caption=Branchless-conditions]
1.0f * (condition) + 0.0f * (1 - condition)
  \end{lstlisting}
  \item To increase performance you can use this to upgrade memory access pattern (use only if you are sure 2 pointers don't point to the same memory):
  \begin{lstlisting}[caption=Restrict access]
float* __restrict__ array
  \end{lstlisting}
  \item In \texttt{animation\_pygame.py} you can see how to load and animate a shader based on time and mouse position.
\end{itemize}


\end{document}